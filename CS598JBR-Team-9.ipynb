{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec83a07",
   "metadata": {},
   "source": [
    "# CS598JBR-Team-9 - MP1: Code Generation Evaluation\n",
    "\n",
    "This notebook implements MP1 for evaluating code generation capabilities of Large Language Models using the HumanEval dataset.\n",
    "\n",
    "**Team Members:**\n",
    "- Saurav Nayak (sgnayak2)\n",
    "- Yegu Sanjana Annamalai (ya11)\n",
    "- Anil Muthigi (muthigi2)\n",
    "- Ritik Hariani (ritikh2)\n",
    "\n",
    "**GitHub Repository:** https://github.com/muthigi2/CS598JBR-Team-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611e81b",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's check if we have GPU access and set up the basic environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61553adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"No CUDA device available. Consider using CPU or getting GPU access.\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/ritikhariani/Repos/CS598JBR-Team-9')\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758f030",
   "metadata": {},
   "source": [
    "## 2. Configure GitHub Repository\n",
    "\n",
    "Update NetIDs and repository information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team NetIDs in alphabetical order\n",
    "NetIDs = [\"muthigi2\", \"ritikh2\", \"sgnayak2\", \"ya11\"]\n",
    "NetIDs_str = \" \".join(NetIDs)\n",
    "\n",
    "print(f\"Team NetIDs: {NetIDs}\")\n",
    "print(f\"NetIDs string: {NetIDs_str}\")\n",
    "\n",
    "# GitHub repository info\n",
    "github_repo = \"https://github.com/muthigi2/CS598JBR-Team-9.git\"\n",
    "print(f\"GitHub Repository: {github_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cebe36",
   "metadata": {},
   "source": [
    "## 3. Install HumanEval Dependencies\n",
    "\n",
    "Run the setup script to install required dependencies for the HumanEval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dataset dependencies\n",
    "!bash -x MP1/setup_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a25c80",
   "metadata": {},
   "source": [
    "## 4. Generate Team Dataset\n",
    "\n",
    "Generate a unique subset of 20 HumanEval problems based on team NetIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for the team\n",
    "dataset_cmd = f\"python3 MP1/dataset_generation.py {NetIDs_str}\"\n",
    "print(f\"Running: {dataset_cmd}\")\n",
    "\n",
    "result = subprocess.run(dataset_cmd.split(), capture_output=True, text=True)\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save log\n",
    "with open('MP1/dataset_generation.log', 'w') as f:\n",
    "    f.write(f\"Command: {dataset_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)\n",
    "\n",
    "print(\"\\nDataset generation completed. Check MP1/dataset_generation.log for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4c95a",
   "metadata": {},
   "source": [
    "## 5. Extract Seed and Set File Names\n",
    "\n",
    "Extract the generated seed from the dataset file and set up file names for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3763c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "# Find the generated dataset file to extract seed\n",
    "dataset_files = glob.glob('selected_humaneval_*.jsonl')\n",
    "if dataset_files:\n",
    "    dataset_file = dataset_files[0]\n",
    "    # Extract seed from filename\n",
    "    match = re.search(r'selected_humaneval_(\\d+)\\.jsonl', dataset_file)\n",
    "    if match:\n",
    "        seed = match.group(1)\n",
    "        print(f\"Generated seed: {seed}\")\n",
    "        print(f\"Dataset file: {dataset_file}\")\n",
    "    else:\n",
    "        print(\"Could not extract seed from filename\")\n",
    "        seed = \"unknown\"\n",
    "else:\n",
    "    print(\"No dataset file found\")\n",
    "    seed = \"unknown\"\n",
    "\n",
    "# Set up file names\n",
    "input_dataset = f\"selected_humaneval_{seed}.jsonl\"\n",
    "base_with_quantization = f\"MP1/base_prompt_{seed}.jsonl\"\n",
    "instruct_with_quantization = f\"MP1/instruct_prompt_{seed}.jsonl\"\n",
    "base_with_quantization_processed = f\"MP1/base_prompt_processed_{seed}.jsonl\"\n",
    "instruct_with_quantization_processed = f\"MP1/instruct_prompt_processed_{seed}.jsonl\"\n",
    "\n",
    "print(f\"\\nFile names set:\")\n",
    "print(f\"Input dataset: {input_dataset}\")\n",
    "print(f\"Base output: {base_with_quantization}\")\n",
    "print(f\"Instruct output: {instruct_with_quantization}\")\n",
    "print(f\"Base processed: {base_with_quantization_processed}\")\n",
    "print(f\"Instruct processed: {instruct_with_quantization_processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5739d6d",
   "metadata": {},
   "source": [
    "## 6. Install Model Dependencies\n",
    "\n",
    "Install the required dependencies for loading and using DeepSeekCoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install model dependencies\n",
    "!bash -x MP1/setup_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b412c5",
   "metadata": {},
   "source": [
    "## 7. Load and Prompt DeepSeekCoder Models\n",
    "\n",
    "⚠️ **GPU Required**: This section requires GPU access to load and run the models.\n",
    "\n",
    "Load both DeepSeekCoder-6.7b-base and DeepSeekCoder-6.7b-instruct models with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the base model\n",
    "base_cmd = f'python3 MP1/model_prompting.py {input_dataset} \"deepseek-ai/deepseek-coder-6.7b-base\" {base_with_quantization} {base_with_quantization_processed} \"True\"'\n",
    "print(f\"Running base model: {base_cmd}\")\n",
    "\n",
    "result = subprocess.run(base_cmd, shell=True, capture_output=True, text=True)\n",
    "print(\"Base model output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Base model errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save base model log\n",
    "with open('MP1/base_prompt.log', 'w') as f:\n",
    "    f.write(f\"Command: {base_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the instruct model\n",
    "instruct_cmd = f'python3 MP1/model_prompting.py {input_dataset} \"deepseek-ai/deepseek-coder-6.7b-instruct\" {instruct_with_quantization} {instruct_with_quantization_processed} \"True\"'\n",
    "print(f\"Running instruct model: {instruct_cmd}\")\n",
    "\n",
    "result = subprocess.run(instruct_cmd, shell=True, capture_output=True, text=True)\n",
    "print(\"Instruct model output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Instruct model errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save instruct model log\n",
    "with open('MP1/instruct_prompt.log', 'w') as f:\n",
    "    f.write(f\"Command: {instruct_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4daab",
   "metadata": {},
   "source": [
    "## 8. Post-process Model Responses\n",
    "\n",
    "The post-processing has already been implemented in the model_prompting.py file. Let's check if the processed files were generated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check if processed files exist and show sample\n",
    "processed_files = [\n",
    "    base_with_quantization_processed,\n",
    "    instruct_with_quantization_processed\n",
    "]\n",
    "\n",
    "for file_path in processed_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✓ {file_path} exists\")\n",
    "        \n",
    "        # Show first entry as sample\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_line = f.readline()\n",
    "            if first_line:\n",
    "                entry = json.loads(first_line)\n",
    "                print(f\"  Sample task_id: {entry.get('task_id', 'N/A')}\")\n",
    "                completion = entry.get('completion', '')\n",
    "                print(f\"  Completion length: {len(completion)} characters\")\n",
    "                print(f\"  First 100 chars: {completion[:100]}...\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"✗ {file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbd275",
   "metadata": {},
   "source": [
    "## 9. Evaluate Generated Code\n",
    "\n",
    "Run the evaluation scripts to test generated code against HumanEval test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model results\n",
    "base_eval_cmd = f\"evaluate_functional_correctness {base_with_quantization}\"\n",
    "print(f\"Evaluating base model: {base_eval_cmd}\")\n",
    "\n",
    "result = subprocess.run(base_eval_cmd.split(), capture_output=True, text=True)\n",
    "print(\"Base evaluation output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Base evaluation errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save base evaluation log\n",
    "with open('MP1/base_evaluate.log', 'w') as f:\n",
    "    f.write(f\"Command: {base_eval_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate instruct model results\n",
    "instruct_eval_cmd = f\"evaluate_functional_correctness {instruct_with_quantization}\"\n",
    "print(f\"Evaluating instruct model: {instruct_eval_cmd}\")\n",
    "\n",
    "result = subprocess.run(instruct_eval_cmd.split(), capture_output=True, text=True)\n",
    "print(\"Instruct evaluation output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Instruct evaluation errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save instruct evaluation log\n",
    "with open('MP1/instruct_evaluate.log', 'w') as f:\n",
    "    f.write(f\"Command: {instruct_eval_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model processed results\n",
    "base_processed_eval_cmd = f\"evaluate_functional_correctness {base_with_quantization_processed}\"\n",
    "print(f\"Evaluating base processed: {base_processed_eval_cmd}\")\n",
    "\n",
    "result = subprocess.run(base_processed_eval_cmd.split(), capture_output=True, text=True)\n",
    "print(\"Base processed evaluation output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Base processed evaluation errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save base processed evaluation log\n",
    "with open('MP1/base_evaluate_processed.log', 'w') as f:\n",
    "    f.write(f\"Command: {base_processed_eval_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate instruct model processed results\n",
    "instruct_processed_eval_cmd = f\"evaluate_functional_correctness {instruct_with_quantization_processed}\"\n",
    "print(f\"Evaluating instruct processed: {instruct_processed_eval_cmd}\")\n",
    "\n",
    "result = subprocess.run(instruct_processed_eval_cmd.split(), capture_output=True, text=True)\n",
    "print(\"Instruct processed evaluation output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Instruct processed evaluation errors:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Save instruct processed evaluation log\n",
    "with open('MP1/instruct_evaluate_processed.log', 'w') as f:\n",
    "    f.write(f\"Command: {instruct_processed_eval_cmd}\\n\")\n",
    "    f.write(\"STDOUT:\\n\")\n",
    "    f.write(result.stdout)\n",
    "    if result.stderr:\n",
    "        f.write(\"\\nSTDERR:\\n\")\n",
    "        f.write(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2af47",
   "metadata": {},
   "source": [
    "## 10. Analyze Results\n",
    "\n",
    "Parse the evaluation results and create analysis tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c728f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def parse_results_file(filepath):\n",
    "    \"\"\"Parse evaluation results file to extract pass/fail information\"\"\"\n",
    "    results = {}\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        task_id = data.get('task_id')\n",
    "                        passed = data.get('passed', False)\n",
    "                        results[task_id] = passed\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    return results\n",
    "\n",
    "def extract_pass_at_k(log_filepath):\n",
    "    \"\"\"Extract pass@k values from evaluation log\"\"\"\n",
    "    pass_at_k = None\n",
    "    if os.path.exists(log_filepath):\n",
    "        with open(log_filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Look for pass@1 in the log\n",
    "            match = re.search(r'pass@1\\s*:\\s*([0-9.]+)', content)\n",
    "            if match:\n",
    "                pass_at_k = float(match.group(1))\n",
    "    return pass_at_k\n",
    "\n",
    "# Parse all result files\n",
    "base_results = parse_results_file(f\"MP1/base_prompt_{seed}.jsonl_results.jsonl\")\n",
    "base_processed_results = parse_results_file(f\"MP1/base_prompt_processed_{seed}.jsonl_results.jsonl\")\n",
    "instruct_results = parse_results_file(f\"MP1/instruct_prompt_{seed}.jsonl_results.jsonl\")\n",
    "instruct_processed_results = parse_results_file(f\"MP1/instruct_prompt_processed_{seed}.jsonl_results.jsonl\")\n",
    "\n",
    "# Extract pass@1 values\n",
    "base_pass_at_1 = extract_pass_at_k('MP1/base_evaluate.log')\n",
    "base_processed_pass_at_1 = extract_pass_at_k('MP1/base_evaluate_processed.log')\n",
    "instruct_pass_at_1 = extract_pass_at_k('MP1/instruct_evaluate.log')\n",
    "instruct_processed_pass_at_1 = extract_pass_at_k('MP1/instruct_evaluate_processed.log')\n",
    "\n",
    "print(\"Pass@1 Results:\")\n",
    "print(f\"Base model: {base_pass_at_1}\")\n",
    "print(f\"Base model (processed): {base_processed_pass_at_1}\")\n",
    "print(f\"Instruct model: {instruct_pass_at_1}\")\n",
    "print(f\"Instruct model (processed): {instruct_processed_pass_at_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f994a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "all_task_ids = set()\n",
    "all_task_ids.update(base_results.keys())\n",
    "all_task_ids.update(base_processed_results.keys())\n",
    "all_task_ids.update(instruct_results.keys())\n",
    "all_task_ids.update(instruct_processed_results.keys())\n",
    "\n",
    "comparison_data = []\n",
    "for task_id in sorted(all_task_ids):\n",
    "    row = {\n",
    "        'problem_ID': task_id,\n",
    "        'base_results': base_results.get(task_id, False),\n",
    "        'base_results_processed': base_processed_results.get(task_id, False),\n",
    "        'instruct_results': instruct_results.get(task_id, False),\n",
    "        'instruct_results_processed': instruct_processed_results.get(task_id, False)\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nPairwise Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('MP1/results_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to MP1/results_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d159c5",
   "metadata": {},
   "source": [
    "## 11. Run Validation Scripts\n",
    "\n",
    "Execute the validation script to ensure all deliverables follow the required structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f309686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation script\n",
    "validation_cmd = f\"python3 MP1/validate.py {github_repo}\"\n",
    "print(f\"Running validation: {validation_cmd}\")\n",
    "\n",
    "result = subprocess.run(validation_cmd.split(), capture_output=True, text=True)\n",
    "print(\"Validation output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Validation errors:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b5422",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### What is the pass@k metric?\n",
    "\n",
    "The pass@k metric measures the percentage of problems for which at least one correct solution is found among k generated solutions. The formula is:\n",
    "\n",
    "**pass@k = (1/n) × Σ(i=1 to n) [1{ci ≥ k}]**\n",
    "\n",
    "Where:\n",
    "- n = total number of problems\n",
    "- ci = number of correct solutions for problem i\n",
    "- 1{ci ≥ k} = indicator function (1 if ci ≥ k, 0 otherwise)\n",
    "\n",
    "For pass@1, it simply measures the percentage of problems solved correctly on the first attempt.\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "This notebook generates all required deliverables:\n",
    "\n",
    "1. **Dataset files**: `selected_humaneval_{seed}.jsonl`, `dataset_generation.log`\n",
    "2. **Model output files**: `base_prompt_{seed}.jsonl`, `instruct_prompt_{seed}.jsonl`\n",
    "3. **Processed files**: `base_prompt_processed_{seed}.jsonl`, `instruct_prompt_processed_{seed}.jsonl`\n",
    "4. **Evaluation results**: All `*_results.jsonl` files\n",
    "5. **Log files**: All required `.log` files\n",
    "6. **Analysis**: `results_comparison.csv`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Commit all generated files to your GitHub repository\n",
    "2. Update the progress report with the analysis\n",
    "3. Include the pass@k values and comparison table\n",
    "4. Analyze patterns in the results and discuss post-processing effectiveness"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
